{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Restaurant Reviews from RestaurantGuru\n",
    "\n",
    "#### Purpose\n",
    "This notebook is the **core data collection component** of the project.\n",
    "It scrapes customer reviews for each restaurant previously identified in the link scraping step.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Notebook Does\n",
    "- Loads restaurant URLs generated by the link scraping notebook\n",
    "- Visits each restaurant profile page\n",
    "- Extracts review titles and review texts\n",
    "- Saves the scraped reviews into multiple CSV files\n",
    "\n",
    "---\n",
    "\n",
    "#### Role in the Project\n",
    "This notebook transforms raw restaurant links into **structured review data**,\n",
    "which forms the main dataset for analysis or downstream applications.\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "- Multiple CSV files containing restaurant reviews\n",
    "- Files are later consolidated using:\n",
    "  `merge_restaurant_reviews.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes\n",
    "- Randomized waiting times are used to mimic human browsing behavior\n",
    "- The process can be time-consuming for large cities\n",
    "- Failed pages are skipped to keep the scraping process robust\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import random\n",
    "from random import uniform\n",
    "import pandas as pd\n",
    "import glob\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----Onedrive BENUTZERNAME (z.B. ElianeTuchborn)\n",
    "username=\"ElianeTuchborn\"\n",
    "\n",
    "#-----Stadt des Link-Datensatzes\n",
    "stadt = \"Frankfurt\"\n",
    "\n",
    "#-----Hier Index für ersten Restaurant-Link der gescraped werden soll festlegen\n",
    "index_start = 415\n",
    "\n",
    "#-----Hier Index für letzten Restaurant-Link der gescraped werden soll festlegen\n",
    "index_stop = 4798\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dateipfad und Dateiname der CSV-Datei\n",
    "dateipfad_input = r\"C:\\Users\\\\\" + username + r\"\\Outputs_Links\\\\\"\n",
    "dateiname_input = \"01Links_\" + stadt + \".csv\"\n",
    "\n",
    "dateipfad_output = r\"C:\\Users\\\\\" + username + r\"\\Outputs_Bewertungen\\\\\"\n",
    "dateiname_output = \"Bewertungen_\" + stadt + \".csv\"\n",
    "\n",
    "\n",
    "# Definiere den Pfad zum Chrome Webdriver\n",
    "options = Options() #erstelle Options-Objekt um m verschiedene Konfigurationsoptionen für den Webdriver anzupassen\n",
    "PATH = \"C:\\\\Users\\\\\" + username + \"\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH, options=options) #definiere Driver\n",
    "\n",
    "\n",
    "# CSV-Datei importieren und als DataFrame speichern\n",
    "df_links = pd.read_csv(dateipfad_input + dateiname_input)\n",
    "list_links = df_links[\"Link\"].tolist()\n",
    "\n",
    "\n",
    "for i in range(index_start, len(list_links)):\n",
    "    if i <= index_stop:\n",
    "        link = list_links[i]\n",
    "        print(i)\n",
    "        print(link)\n",
    "        try:\n",
    "            # Erstelle eine Instanz der Chrome-Optionen\n",
    "            options = Options()\n",
    "            options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Deaktiviert bestimmte Blink-Funktionen im Browser, die von Automatisierungswerkzeugen verwendet werden können.\n",
    "            options.add_argument(\"--disable-extensions\")  # Deaktiviert Browser-Erweiterungen, um mögliche Konflikte oder unerwünschte Verhaltensweisen zu vermeiden.\n",
    "            options.add_argument(\"--disable-infobars\")  # Deaktiviert Infobalken im Browser, die Informationen anzeigen können.\n",
    "            options.add_argument(\"--disable-notifications\")  # Deaktiviert Browser-Benachrichtigungen, um Unterbrechungen während des Automatisierungsprozesses zu vermeiden.\n",
    "            options.add_argument(\"--disable-popup-blocking\")  # Deaktiviert das Blockieren von Popup-Fenstern im Browser.\n",
    "            options.add_argument(\"--disable-web-security\")  # Deaktiviert die Web-Sicherheitseinstellungen im Browser, um den Zugriff auf Ressourcen von verschiedenen Domains zu ermöglichen.\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")  # Deaktiviert die gemeinsame Nutzung des /dev/shm-Speichers im Browser, was die Speichernutzung reduzieren kann.\n",
    "            options.add_argument(\"--disable-gpu\")  # Deaktiviert die Hardwarebeschleunigung im Browser.\n",
    "            options.add_argument(\"--window-size=1200,800\")  # Setzt die Größe des Browserfensters auf 1200 Pixel Breite und 800 Pixel Höhe.\n",
    "            options.add_argument(\"--disable-features=VizDisplayCompositor\")  # Deaktiviert das Anzeigen von Inhalten mit dem VizDisplayCompositor.\n",
    "            options.add_argument(\"--window-size=1366,768\")  # Setzt die Größe des Browserfensters auf 1366 Pixel Breite und 768 Pixel Höhe.\n",
    "\n",
    "            # headless Browser\n",
    "            #options.add_argument(\"--headless\")\n",
    "\n",
    "            # Warte\n",
    "            time.sleep(uniform(1, 3))\n",
    "\n",
    "            # Öffne die Website\n",
    "            driver.get(link)\n",
    "\n",
    "            # Warte\n",
    "            time.sleep(uniform(1, 3))\n",
    "\n",
    "            # Scrolle bis ans Ende der Seite\n",
    "            current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            while True:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(uniform(1, 2))\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == current_height:\n",
    "                    break\n",
    "                current_height = new_height\n",
    "\n",
    "            # Warte\n",
    "            time.sleep(uniform(3, 4))\n",
    "\n",
    "            \n",
    "            # Finde alle Elemente mit Kommentaren, Bewertungen, Benutzernamen, ID und Datum\n",
    "            container = driver.find_element_by_xpath(\"/html/body/div[3]/div[1]/div[1]/div/div[4]\") # Definiere Container; X-Path gibt spezifische Funktion des div-Elements an\n",
    "\n",
    "            comments = container.find_elements_by_class_name(\"text_full\")\n",
    "            elements = container.find_elements_by_class_name(\"user_info__head\")\n",
    "            scores = container.find_elements_by_css_selector(\"[data-score]\")  # Finde Elemente mit \"data-score\" Attribut\n",
    "            id_elements = container.find_elements_by_css_selector(\"[data-id]\")  # Finde Elemente mit \"data-id\" Attribut\n",
    "\n",
    "            # Extrahierte Kommentare in Datensatz\n",
    "            dfcomment = pd.DataFrame([comment.text for comment in comments], columns=[\"Kommentar\"])\n",
    "\n",
    "            # Extrahierte Elemente in Datensatz und splitte nach Benutzer, Datum und Plattform\n",
    "            element_list = [element.text for element in elements]\n",
    "            dfelement = pd.DataFrame(element_list, columns=[\"Element\"])\n",
    "            dfelement[[\"Name\", \"Datum\", \"Plattform\"]] = dfelement[\"Element\"].str.split(\" vor | auf \", expand=True)\n",
    "\n",
    "            # Extrahiere den Wert des \"data-score\" Attributs als Text\n",
    "            dfscores = pd.DataFrame([score.get_attribute(\"data-score\") for score in scores], columns=[\"Score\"])\n",
    "\n",
    "            # Extrahiere den Wert des \"data-id\" Attributs als Text\n",
    "            dfid = pd.DataFrame([id_element.get_attribute(\"data-id\") for id_element in id_elements], columns=[\"ID\"])\n",
    "\n",
    "            # Datensätze horizontal aneinanderhängen\n",
    "            df_bewertungen = pd.concat([dfcomment, dfelement, dfscores, dfid], axis=1)\n",
    "\n",
    "        \n",
    "\n",
    "            #Zeitangeben cleanen\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"1 Monate\", \"04.23\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"2 Monate\", \"03.23\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"3 Monate\", \"02.23\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"4 Monate\", \"01.23\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"5 Monate\", \"12.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"6 Monate\", \"11.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"7 Monate\", \"10.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"8 Monate\", \"09.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"9 Monate\", \"08.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"10 Monate\", \"07.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"11 Monate\", \"06.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"12 Monate\", \"05.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"ein Jahr\", \"00.22\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"2 Jahre\", \"00.21\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"3 Jahre\", \"00.20\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"4 Jahre\", \"00.19\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"5 Jahre\", \"00.18\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"6 Jahre\", \"00.17\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"7 Jahre\", \"00.16\")\n",
    "            df_bewertungen[\"Datum\"]=df_bewertungen[\"Datum\"].replace(\"8 Jahre\", \"00.15\")\n",
    "\n",
    "\n",
    "            # Speichern\n",
    "            filename = f\"01Bewertungen_{stadt}{i}.csv\"\n",
    "            df_bewertungen.to_csv(os.path.join(dateipfad_output, filename), index=False)\n",
    "\n",
    "            # Zeige Fortschritt an\n",
    "            print(\"Restaurant\", i , \"von\", len(list_links), \"scrape done.\")\n",
    "\n",
    "            # Schließe Browser\n",
    "            #driver.quit()\n",
    "            time.sleep(uniform(15, 24))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Fehler:\", e)\n",
    "\n",
    "\n",
    "            # Speichere bisher gesammelte Daten als CSV\n",
    "            #filename = f\"Bewertungen_{stadt}_{index_start}_{i-1}.csv\"\n",
    "            #df_bewertungen.to_csv(os.path.join(dateipfad_output, filename), index=False)\n",
    "\n",
    "    \n",
    "            break\n",
    "    else: print(\"Alle Restaurants wurden gescraped\")    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
